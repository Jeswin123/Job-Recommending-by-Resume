{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d553c3ef-93db-4db2-b93d-472fd06d935f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.2.6)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\hp\\anaconda3\\lib\\site-packages (14.0.2)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Downloading pyarrow-21.0.0-cp312-cp312-win_amd64.whl (26.2 MB)\n",
      "   ---------------------------------------- 0.0/26.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/26.2 MB 5.6 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.1/26.2 MB 5.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 3.1/26.2 MB 5.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.2/26.2 MB 5.2 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 5.0/26.2 MB 5.0 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 5.8/26.2 MB 4.9 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 6.8/26.2 MB 4.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.9/26.2 MB 4.9 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 8.7/26.2 MB 4.9 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 10.0/26.2 MB 4.9 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 11.0/26.2 MB 4.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 12.3/26.2 MB 5.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 13.4/26.2 MB 5.0 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 14.7/26.2 MB 5.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 15.7/26.2 MB 5.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 16.8/26.2 MB 5.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 18.1/26.2 MB 5.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 19.1/26.2 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 20.4/26.2 MB 5.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 21.5/26.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.8/26.2 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.9/26.2 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.9/26.2 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.0/26.2 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.2/26.2 MB 5.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "Successfully installed pyarrow-21.0.0\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade numpy scikit-learn pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "834e0a12-d88c-4256-aace-b18e6a79afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Libraries\n",
    "\n",
    "import PyPDF2\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9cf0ff7-f023-4b09-8840-96dc0a0c2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy English model\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b49c4dc-4613-408f-84b4-c038ff725ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Function to Extract Text from Resume PDF\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70bdfe72-5995-43e7-9ddf-0ae8c7a31df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#3. Split Resume into Sections to Understand the Resume Templeate\n",
    "\n",
    "def split_sections(text):\n",
    "    text = text.lower()\n",
    "    sections = {\n",
    "        \"skills\": \"\",\n",
    "        \"education\": \"\",\n",
    "        \"experience\": \"\"\n",
    "    }\n",
    "\n",
    "    pattern = r\"(skills|education|academic background|experience|work experience|internship|projects)\"\n",
    "    matches = list(re.finditer(pattern, text))\n",
    "\n",
    "    for i in range(len(matches)):\n",
    "        start = matches[i].end()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "        section_title = matches[i].group().strip()\n",
    "        content = text[start:end].strip()\n",
    "\n",
    "        if \"skill\" in section_title:\n",
    "            sections[\"skills\"] += content\n",
    "        elif \"education\" in section_title or \"academic\" in section_title:\n",
    "            sections[\"education\"] += content\n",
    "        elif \"experience\" in section_title or \"internship\" in section_title:\n",
    "            sections[\"experience\"] += content\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "618e2d38-03c1-40c7-9b6c-bd6c5b197c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extract Informations from Each Section\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def extract_skills(text):\n",
    "    doc = nlp(text)\n",
    "    skills = set()\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if len(chunk.text.split()) <= 4:\n",
    "            skills.add(chunk.text.strip().lower())\n",
    "    return list(skills)\n",
    "\n",
    "def extract_education(text):\n",
    "    # Use regex to match degree-like patterns\n",
    "    patterns = r\"(b\\.tech|btech|m\\.tech|mtech|b\\.sc|m\\.sc|mba|phd|bachelor|master|ba|ma)\"\n",
    "    matches = re.findall(patterns, text, re.IGNORECASE)\n",
    "    return list(set([m.lower() for m in matches])) if matches else [\"Not found\"]\n",
    "\n",
    "def extract_experience(text):\n",
    "    match = re.search(r'(\\d+)\\s+years?', text)\n",
    "    return match.group(0) if match else \"Not mentioned\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca823819-f42f-4a7a-a233-20c12e6cff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.Training of Job Role Classifier\n",
    "\n",
    "def train_classifier(dataset_path):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    df['resume_text'] = df['resume_text'].apply(clean_text)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('clf', LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(df['resume_text'], df['job_role'])\n",
    "    joblib.dump(pipeline, \"job_role_classifier.joblib\")\n",
    "    print(\"!!!Model trained and saved.!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d1e1a3b-a0d4-4b48-93c9-e145fc5486db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Predict N-Job Role According to Ranking Based on the Skills \n",
    "\n",
    "def predict_job_role(resume_path, model_path=\"job_role_classifier.joblib\",top_n=5):\n",
    "    model = joblib.load(model_path)\n",
    "    raw_text = extract_text_from_pdf(resume_path)\n",
    "    sections = split_sections(raw_text)\n",
    "\n",
    "#Extract details from resume sections\n",
    "    skills = extract_skills(sections['skills'])\n",
    "    education = extract_education(sections['education'])\n",
    "    experience = extract_experience(sections['experience'])\n",
    "\n",
    "#Predict top N roles\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    probabilities = model.predict_proba([cleaned_text])[0]\n",
    "    job_roles = model.classes_\n",
    "\n",
    "    top_indices = probabilities.argsort()[-top_n:][::-1]\n",
    "    top_roles = [(job_roles[i], round(probabilities[i] * 100, 2)) for i in top_indices]\n",
    "\n",
    "#Final Output\n",
    "    print(\"\\nðŸ“„ Extracted Resume Details:\")\n",
    "    print(\"Skills     :\", skills if skills else \"Not found\")\n",
    "    print(\"Education  :\", education)\n",
    "    print(\"Experience :\", experience)\n",
    "    print(\"\\n Top {top_n} Predicted Job Roles:\")\n",
    "    for role, score in top_roles:\n",
    "        print(f\"- {role} ({score}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7bb99b3-7f5b-4259-ad52-a5db563ea589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model trained and saved.\n",
      "\n",
      "ðŸ“„ Extracted Resume Details:\n",
      "Skills     : ['postman api', 'keras', 'rest apis', 'r programming', 'ethic,\\nresponsibility', 'tableau', 'that', 'professional growth', 'ineterests', 'a dynamic work environment', 'adaptability', 'innovation', 'ml', 'ms\\nexcel', 'power bi', 'deep\\nlearning', 'the companyÊ¼s success', 'django rest framework', 'nlp\\nsoft: team work', 'pandas', 'ai', 'tensor flow', 'a role', 'english malayalam hindi', 'nltk', 'creativity\\nlanguages', 'neural networks', 'my sql', 'python', 'c programming', 'data analysis', 'django']\n",
      "Education  : ['b.tech', 'ma']\n",
      "Experience : Not mentioned\n",
      "\n",
      " Top {top_n} Predicted Job Roles:\n",
      "- Backend Developer (17.59%)\n",
      "- Data Analyst (15.05%)\n",
      "- Accountant (14.0%)\n",
      "- Data Engineer (14.0%)\n",
      "- ML Engineer (13.79%)\n"
     ]
    }
   ],
   "source": [
    "# 6. Main Entry\n",
    "\n",
    "train_classifier(r\"C:\\Users\\HP\\Downloads\\job_role_dataset.csv\") \n",
    "predict_job_role(r\"C:\\Users\\HP\\Downloads\\Jeswin-P-Vincent-Resume.pdf\", top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c949711-9981-4e53-a18a-64aec47f7f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca89d1-cbdb-4302-a6c2-460e48a6a81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b993a-a05c-4d51-9bd3-3cfe493c82cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c32681-b7e6-4e65-94d1-7ba66103f662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a96920-6fbe-4161-981e-fd100ac499b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed5deb-df50-43ad-9869-3e531e78c7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c0d8a-3b5b-405e-8201-21f41d4a863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------\n",
    "\n",
    "# --------------------------------------\n",
    "\n",
    "# --------------------------------------\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
